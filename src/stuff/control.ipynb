{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "import threading\n",
    "import numpy as np\n",
    "import subprocess\n",
    "import re"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- `metadata_renamed` - то что делал Влад (переделанные наши данные)\n",
    "- `metadata_old` - категории рутуба\n",
    "- `metadata_orig` - категории рутуба + то, что делал Влад\n",
    "- `metadata_filtered` - metadata_orig с отфильтрованными категориями по встречаемости"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE = '/mnt/nfs'\n",
    "\n",
    "metadata_r = pd.read_csv(f'{BASE}/data/metadata_old.csv')\n",
    "metadata_o = pd.read_csv(f'{BASE}/data/metadata_renamed.csv')\n",
    "\n",
    "categories = pd.read_csv(f'{BASE}/data/train_data_categories.csv').dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def categories_extraction(x):\n",
    "    x = x.split(',')\n",
    "    x = [re.split(r'[:\\t]', cat)[0].strip() for cat in x]\n",
    "    x = ', '.join(set(x))\n",
    "    return x\n",
    "\n",
    "categories['lvl0'] = categories.tags.apply(categories_extraction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_categories = []\n",
    "for i in categories.lvl0:\n",
    "    i = list(filter(bool, i.split(', ')))\n",
    "    all_categories.extend(i)\n",
    "\n",
    "all_categories = set(all_categories)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Merge videos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/7220 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 7220/7220 [00:01<00:00, 6108.13it/s]\n",
      "100%|██████████| 1675/1675 [00:00<00:00, 2162.76it/s]\n"
     ]
    }
   ],
   "source": [
    "ids_to_delete = set()\n",
    "\n",
    "# объединяем датасеты\n",
    "for video_id in tqdm(metadata_o.video_id):\n",
    "    audio_path = f'{BASE}/parsed_data/audio/{video_id}.mp3'\n",
    "    frames_path = f'{BASE}/parsed_data/frames/{video_id}/'\n",
    "\n",
    "    if not os.path.exists(frames_path) or not os.path.isdir(frames_path) or len(os.listdir(frames_path)) < 64:\n",
    "        ids_to_delete.add(video_id)\n",
    "    if not os.path.exists(audio_path) or os.path.getsize(audio_path) == 0:\n",
    "        ids_to_delete.add(video_id)\n",
    "\n",
    "metadata_o = metadata_o[~metadata_o.video_id.isin(ids_to_delete)]\n",
    "metadata = pd.concat((metadata_o, metadata_r))\n",
    "\n",
    "# перемещаем видео и аудио\n",
    "for video_id in tqdm(metadata_o.video_id):\n",
    "    audio_path = f'{BASE}/parsed_data/audio/{video_id}.mp3'\n",
    "    new_audio_path = f'{BASE}/data/audio/{video_id}.mp3'\n",
    "\n",
    "    if not os.path.exists(new_audio_path) or os.path.getsize(new_audio_path) == 0:\n",
    "        audio_cp = f'cp {audio_path} {new_audio_path}'\n",
    "        subprocess.run(audio_cp, shell=True)\n",
    "    \n",
    "    frames_path = f'{BASE}/parsed_data/frames/{video_id}/'\n",
    "    new_frames_path = f'{BASE}/data/frames/{video_id}/'\n",
    "\n",
    "    if not os.path.exists(new_frames_path) or not os.path.isdir(new_frames_path) or len(os.listdir(new_frames_path)) < 64:\n",
    "        frames_cp = f'cp -r {frames_path} {new_frames_path}'\n",
    "        subprocess.run(frames_cp, shell=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata.to_csv(f'{BASE}/data/metadata_orig.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filter by frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata = pd.read_csv(f'{BASE}/data/metadata_orig.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# удаляем видео, которые не соответствуют категориям рутуба\n",
    "indices_to_delete = []\n",
    "for i in range(len(metadata)):\n",
    "    row = metadata.iloc[i]\n",
    "    cats = row['category']\n",
    "    cats = cats.split(', ')\n",
    "    for cat in cats:\n",
    "        if cat not in all_categories:\n",
    "            indices_to_delete.append(i)\n",
    "\n",
    "# обрезаем часто встречающиеся категории\n",
    "all_categories = {}\n",
    "for i in range(len(metadata)):\n",
    "    row = metadata.iloc[i]\n",
    "    categories = row['category'].split(', ')\n",
    "    for cat in categories:\n",
    "        if not cat in all_categories:\n",
    "            all_categories[cat] = 1\n",
    "        elif all_categories[cat] < 100:\n",
    "            all_categories[cat] += 1\n",
    "        else:\n",
    "            indices_to_delete.append(i)\n",
    "\n",
    "# удаляем редко встрчающиеся категории\n",
    "for i in range(len(metadata)):\n",
    "    row = metadata.iloc[i]\n",
    "    categories = row['category'].split(', ')\n",
    "    save=True\n",
    "    for cat in categories:\n",
    "        if all_categories[cat] < 10:\n",
    "            save=False\n",
    "    if not save:\n",
    "        indices_to_delete.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_indices = list(set(list(metadata.index)) - set(indices_to_delete))\n",
    "metadata_filtered = metadata.iloc[filtered_indices]\n",
    "metadata_filtered.to_csv(f'{BASE}/data/metadata_filtered.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1648, 5)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "metadata_filtered.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Categories + Tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata_stacked = pd.read_csv(f'{BASE}/data/metadata_orig.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata_stacked['tag'] = metadata_stacked['tag'] + ', ' + metadata_stacked['category']\n",
    "\n",
    "metadata_stacked.to_csv(f'{BASE}/data/metadata_stacked.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Try train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/NaRuTagAIback/venv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "2024-09-28 10:06:46.456273: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F AVX512_VNNI FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-09-28 10:06:46.629262: I tensorflow/core/util/util.cc:169] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2024-09-28 10:06:46.677619: E tensorflow/stream_executor/cuda/cuda_blas.cc:2981] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2024-09-28 10:06:47.556580: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n",
      "2024-09-28 10:06:47.556694: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n",
      "2024-09-28 10:06:47.556704: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n",
      "[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from functools import partial\n",
    "from torch.utils.data import DataLoader\n",
    "from src.modelling.video_dataset import get_datasets, collate_fn\n",
    "\n",
    "def get_loaders(path_to_data, val_size, test_size, seed, list_no_include_cat, batch_size, use_text_augmentation, \n",
    "                use_text_lematization, num_workers, pin_memory):\n",
    "    # Определяем датасеты\n",
    "    train_dataset, valid_dataset, test_dataset = get_datasets(path_to_data,\n",
    "                                                              val_size=val_size,\n",
    "                                                              test_size=test_size,\n",
    "                                                              seed=seed,\n",
    "                                                              categories=list_no_include_cat)\n",
    "\n",
    "    # Инициализируем DataLoader для тренировочного набора\n",
    "    train_loader = DataLoader(train_dataset,\n",
    "                              batch_size=batch_size,\n",
    "                              shuffle=True,\n",
    "                              collate_fn=partial(collate_fn,\n",
    "                                                 use_augmentation=use_text_augmentation,\n",
    "                                                 use_lemmatization=use_text_lematization),\n",
    "                              num_workers=num_workers,\n",
    "                              pin_memory=pin_memory)\n",
    "\n",
    "    # Инициализируем DataLoader для валидационного набора\n",
    "    valid_loader = DataLoader(valid_dataset,\n",
    "                              batch_size=batch_size,\n",
    "                              shuffle=False,\n",
    "                              collate_fn=collate_fn,\n",
    "                              num_workers=num_workers,\n",
    "                              pin_memory=pin_memory)\n",
    "\n",
    "    # Инициализируем DataLoader для тестового набора\n",
    "    test_loader = DataLoader(test_dataset,\n",
    "                             batch_size=batch_size,\n",
    "                             shuffle=False,\n",
    "                             collate_fn=collate_fn,\n",
    "                             num_workers=num_workers,\n",
    "                             pin_memory=pin_memory)\n",
    "\n",
    "    # Определяем классы\n",
    "    classes = train_dataset.all_categories\n",
    "    num_classes = len(classes)\n",
    "\n",
    "    return train_loader, valid_loader, test_loader, classes, num_classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_labels(labels, num_classes, device):\n",
    "    # Преобразование меток в one-hot encoding\n",
    "    one_hot_labels = torch.zeros(len(labels), num_classes).to(device)\n",
    "    for idx, label in enumerate(labels):\n",
    "        one_hot_labels[idx, label] = 1.0  # Установка 1 для класса метки\n",
    "    return one_hot_labels\n",
    "\n",
    "def decode_labels(matrix, idx2cat):\n",
    "    categories = []\n",
    "    for row in matrix:\n",
    "        # Получаем индексы, где значение 1\n",
    "        indices = torch.nonzero(row).flatten().tolist()\n",
    "        # Преобразуем индексы в категории\n",
    "        cat_list = [idx2cat[idx] for idx in indices]\n",
    "        categories.append(cat_list)\n",
    "    return categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">10:08:09-903525 </span><span style=\"color: #000080; text-decoration-color: #000080\">INFO    </span> VAL INFO                                                                                  \n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                </span>         Total: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">23</span> categories and <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">23</span> tags                                                          \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2;36m10:08:09-903525\u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m VAL INFO                                                                                  \n",
       "\u001b[2;36m                \u001b[0m         Total: \u001b[1;36m23\u001b[0m categories and \u001b[1;36m23\u001b[0m tags                                                          \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">10:08:09-915296 </span><span style=\"color: #000080; text-decoration-color: #000080\">INFO    </span> TRAIN INFO                                                                                \n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                </span>         Total: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">31</span> categories and <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">31</span> tags                                                          \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2;36m10:08:09-915296\u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m TRAIN INFO                                                                                \n",
       "\u001b[2;36m                \u001b[0m         Total: \u001b[1;36m31\u001b[0m categories and \u001b[1;36m31\u001b[0m tags                                                          \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">10:08:09-919931 </span><span style=\"color: #000080; text-decoration-color: #000080\">INFO    </span> TEST INFO                                                                                 \n",
       "<span style=\"color: #7fbfbf; text-decoration-color: #7fbfbf\">                </span>         Total: <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">28</span> categories and <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">28</span> tags                                                          \n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[2;36m10:08:09-919931\u001b[0m\u001b[2;36m \u001b[0m\u001b[34mINFO    \u001b[0m TEST INFO                                                                                 \n",
       "\u001b[2;36m                \u001b[0m         Total: \u001b[1;36m28\u001b[0m categories and \u001b[1;36m28\u001b[0m tags                                                          \n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_loader, valid_loader, test_loader, classes, num_classes = get_loaders(\n",
    "    path_to_data='/mnt/nfs/data',\n",
    "    val_size=0.2, test_size=0.2, seed=17, list_no_include_cat=[], batch_size=1, use_text_augmentation=1, \n",
    "                use_text_lematization=1, num_workers=2, pin_memory=1\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/NaRuTagAIback/venv/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1617: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be deprecated in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModel\n",
    "import torch.nn as nn\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"cointegrated/rubert-tiny2\")\n",
    "model = AutoModel.from_pretrained(\"cointegrated/rubert-tiny2\").to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"></pre>\n"
      ],
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "\u001b[1;35mtensor\u001b[0m\u001b[1m(\u001b[0m\u001b[1;36m356746.1875\u001b[0m, \u001b[33mdevice\u001b[0m=\u001b[32m'cuda:0'\u001b[0m, \u001b[33mgrad_fn\u001b[0m=\u001b[1m<\u001b[0m\u001b[1;95mDivBackward1\u001b[0m\u001b[1m>\u001b[0m\u001b[1m)\u001b[0m"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
